# GLTCH

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—                                     â•‘
â•‘   â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•‘  â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘                                     â•‘
â•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘                                     â•‘
â•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘                                     â•‘
â•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘                                     â•‘
â•‘    â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•    â•šâ•â•â•â•â•â•â•šâ•â•  â•šâ•â•                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Generative Language Transformer with Contextual Hierarchy**

Build and train your own language model from scratch. Supports multiple model sizes and distributed training across multiple GPUs.

## Features

- ğŸ§  **Multiple model sizes** â€” 2.7M, 10M, 25M, or 50M parameters
- ğŸ“Š **Live training dashboard** â€” Real-time loss curve and generated samples
- ğŸ’¬ **Chat interface** â€” Talk to your trained model with voice output
- ğŸŒ **Distributed training (Hive)** â€” Train across multiple machines/GPUs
- ğŸ® **Works on consumer GPUs** â€” Or even CPU (just slower)

---

## Quick Start

### Option 1: Train with Dashboard (Recommended)

```bash
git clone https://github.com/cyberdreadx/gltch-2.7m.git
cd gltch-2.7m
pip install torch requests

# Train 2.7M model with live dashboard
python train_with_ui.py

# Or train a larger model
python train_with_ui.py --size 10m
```

Opens a browser dashboard showing loss curve, speed, ETA, and generated samples.

### Option 2: Chat with Trained Model

```bash
pip install torch requests
python chat.py
```

Opens a chat interface at `http://localhost:8889` with:
- ğŸ’¬ Text generation from prompts
- ğŸ”Š Text-to-speech (toggle on/off)
- ğŸšï¸ Adjustable temperature, top-k, repetition penalty

---

## Model Sizes

| Size | Params | VRAM | Training Time (GPU) |
|------|--------|------|---------------------|
| `2.7m` | 2.7M | ~1GB | ~5 min |
| `10m` | ~10M | ~2GB | ~15 min |
| `25m` | ~25M | ~4GB | ~30 min |
| `50m` | ~50M | ~8GB | ~1 hour |

```bash
# Examples
python train_with_ui.py --size 2.7m   # Default
python train_with_ui.py --size 10m    # Larger
python train_with_ui.py --size 50m    # Largest
```

---

## Training Scripts

| Script | Purpose |
|--------|---------|
| `train_with_ui.py` | Dashboard UI + training |
| `train_continuous.py` | Resume training from checkpoint |
| `train_custom.py` | Train on your own text data |
| `train_pro.py` | CLI training with size selection |
| `gltch_2_7m.py` | Simple terminal training |

### Train on Custom Data

```bash
python train_custom.py --data your_novel.txt
python train_custom.py --data ./my_dataset/ --steps 10000
python train_custom.py --data https://example.com/text.txt
```

### Resume Training

```bash
python train_continuous.py --resume
python train_continuous.py --resume --steps 5000  # Add more steps
```

---

## Chat Interface

```bash
python chat.py
```

Open `http://localhost:8889` in your browser.

### Controls

| Slider | What It Does | Default |
|--------|--------------|---------|
| Tokens | Output length | 200 |
| Temp | Creativity (lower = focused) | 0.8 |
| Top-K | Only consider top K tokens | 40 |
| Rep Pen | Penalize repetition | 1.1 |

Click the **ğŸ”Š Voice** button to enable text-to-speech.

---

## Distributed Training (GLTCH Hive)

Train across multiple machines using the Hive network.

### Architecture

```
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  Coordinator    â”‚
                     â”‚  (VPS/Server)   â”‚
                     â”‚  server.py      â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Peer 1      â”‚    â”‚   Peer 2      â”‚    â”‚   Peer 3      â”‚
â”‚   RTX 4090    â”‚    â”‚   RTX 3080    â”‚    â”‚   M1 Mac      â”‚
â”‚   peer.py     â”‚    â”‚   peer.py     â”‚    â”‚   peer.py     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 1: Start Coordinator (on VPS)

```bash
# SSH to your VPS
curl -sSL https://raw.githubusercontent.com/cyberdreadx/gltch-2.7m/main/hive/setup_coordinator.sh | bash
```

Or manually:

```bash
git clone https://github.com/cyberdreadx/gltch-2.7m.git
cd gltch-2.7m/hive
pip install websockets
python server.py
```

Dashboard: `http://YOUR_VPS_IP:8080`

### Step 2: Connect Peers (on GPU machines)

```bash
git clone https://github.com/cyberdreadx/gltch-2.7m.git
cd gltch-2.7m
pip install torch websockets requests

# Connect and train
python hive/peer.py --server ws://YOUR_VPS_IP:8765 --name my-gpu --size 10m
```

### Peer Options

```bash
python hive/peer.py \
    --server ws://coordinator.example.com:8765 \
    --name office-4090 \
    --size 25m
```

| Option | Description |
|--------|-------------|
| `--server` | Coordinator WebSocket URL |
| `--name` | Your peer's display name |
| `--size` | Model size (2.7m/10m/25m/50m) |

---

## Project Structure

```
gltch-2.7m/
â”œâ”€â”€ gltch_2_7m.py          # Core model + terminal training
â”œâ”€â”€ gltch_2_7m_colab.py    # Google Colab version
â”œâ”€â”€ train_with_ui.py       # Dashboard training (--size support)
â”œâ”€â”€ train_continuous.py    # Resumable training
â”œâ”€â”€ train_custom.py        # Train on custom data
â”œâ”€â”€ train_pro.py           # CLI training with sizes
â”œâ”€â”€ chat.py                # Chat interface + voice
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ hive/                  # Distributed training
    â”œâ”€â”€ server.py          # Coordinator
    â”œâ”€â”€ peer.py            # Training peer (--size support)
    â”œâ”€â”€ quick_peer.py      # Easy peer connect
    â”œâ”€â”€ setup_coordinator.sh
    â”œâ”€â”€ index.html         # Dashboard
    â”œâ”€â”€ style.css
    â””â”€â”€ hive.js
```

---

## Model Architecture

```
GLTCH
â”œâ”€â”€ Token Embedding
â”œâ”€â”€ Position Embedding
â”œâ”€â”€ N Ã— Transformer Blocks
â”‚   â”œâ”€â”€ Multi-Head Self-Attention
â”‚   â”œâ”€â”€ Layer Norm
â”‚   â”œâ”€â”€ Feed Forward (GELU)
â”‚   â””â”€â”€ Layer Norm
â”œâ”€â”€ Final Layer Norm
â””â”€â”€ Output Head
```

| Size | Layers | Heads | Dim | Context |
|------|--------|-------|-----|---------|
| 2.7M | 6 | 6 | 192 | 128 |
| 10M | 8 | 8 | 384 | 256 |
| 25M | 12 | 8 | 512 | 512 |
| 50M | 12 | 12 | 768 | 512 |

---

## Requirements

- Python 3.8+
- PyTorch 2.0+
- `requests` (for data loading)
- `websockets` (for Hive only)

```bash
pip install torch requests websockets
```

---

## Google Colab (Free GPU)

1. Open [Google Colab](https://colab.research.google.com)
2. Upload `gltch_2_7m_colab.py`
3. **Runtime â†’ Change runtime type â†’ T4 GPU**
4. Run cells in order

Training takes ~5 minutes on a free T4 GPU.

---

## License

MIT License â€” see [LICENSE](LICENSE)

## Author

Created by **cyberdreadx**

---

*GLTCH â€” Generative Language Transformer with Contextual Hierarchy*
