{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ Join the GLTCH Hive\n",
                "\n",
                "**Contribute your GPU to train AI together!**\n",
                "\n",
                "This notebook connects to the GLTCH distributed training network. Your GPU will help train open-source language models alongside other contributors.\n",
                "\n",
                "---\n",
                "\n",
                "## Quick Start\n",
                "\n",
                "1. **Runtime â†’ Change runtime type â†’ T4 GPU** (or better)\n",
                "2. **Run all cells** (Ctrl+F9 or Runtime â†’ Run all)\n",
                "3. Watch your GPU contribute to training! ğŸš€"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch websockets requests -q\n",
                "print(\"âœ… Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Check GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    gpu_name = torch.cuda.get_device_name(0)\n",
                "    print(f\"ğŸ® GPU detected: {gpu_name}\")\n",
                "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "else:\n",
                "    print(\"âš ï¸ No GPU detected. Go to Runtime â†’ Change runtime type â†’ T4 GPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Download Peer Script"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!wget -q https://raw.githubusercontent.com/cyberdreadx/gltch-llm/main/hive/peer.py -O peer.py\n",
                "print(\"âœ… Peer script downloaded!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Configuration\n",
                "\n",
                "Customize your peer settings below:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CONFIGURATION - Customize these settings!\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "\n",
                "# Your peer's display name (shown on dashboard)\n",
                "PEER_NAME = \"colab-contributor\"\n",
                "\n",
                "# Model size: '1m' (fastest), '2.7m', '10m', '25m', '50m' (largest)\n",
                "MODEL_SIZE = \"10m\"\n",
                "\n",
                "# Server connection (public hive)\n",
                "SERVER_URL = \"ws://76.13.121.10:8765\"\n",
                "AUTH_KEY = \"PUBLIC_KEY\"\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "\n",
                "print(f\"ğŸ“‹ Configuration:\")\n",
                "print(f\"   Name: {PEER_NAME}\")\n",
                "print(f\"   Model: GLTCH-{MODEL_SIZE.upper()}\")\n",
                "print(f\"   Server: {SERVER_URL}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Join the Hive! ğŸ\n",
                "\n",
                "Run this cell to start training. Your GPU will:\n",
                "- Connect to the coordinator\n",
                "- Download training data\n",
                "- Train and share gradients with other peers\n",
                "\n",
                "**Watch the live dashboard:** https://hive.gltch.app"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!python peer.py --server {SERVER_URL} --key {AUTH_KEY} --name {PEER_NAME} --size {MODEL_SIZE}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ğŸ§ª Test the Model\n",
                "\n",
                "After training, you can chat with the model! Run the cells below to generate text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "class Head(nn.Module):\n",
                "    def __init__(self, head_size, n_embd, block_size, dropout):\n",
                "        super().__init__()\n",
                "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
                "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
                "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
                "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x):\n",
                "        B, T, C = x.shape\n",
                "        k, q = self.key(x), self.query(x)\n",
                "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
                "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
                "        wei = self.dropout(F.softmax(wei, dim=-1))\n",
                "        return wei @ self.value(x)\n",
                "\n",
                "class MultiHeadAttention(nn.Module):\n",
                "    def __init__(self, n_heads, head_size, n_embd, block_size, dropout):\n",
                "        super().__init__()\n",
                "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout) for _ in range(n_heads)])\n",
                "        self.proj = nn.Linear(n_embd, n_embd)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "    def forward(self, x):\n",
                "        return self.dropout(self.proj(torch.cat([h(x) for h in self.heads], dim=-1)))\n",
                "\n",
                "class FeedForward(nn.Module):\n",
                "    def __init__(self, n_embd, dropout):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(nn.Linear(n_embd, 4*n_embd), nn.ReLU(), nn.Linear(4*n_embd, n_embd), nn.Dropout(dropout))\n",
                "    def forward(self, x): return self.net(x)\n",
                "\n",
                "class Block(nn.Module):\n",
                "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
                "        super().__init__()\n",
                "        self.sa = MultiHeadAttention(n_head, n_embd//n_head, n_embd, block_size, dropout)\n",
                "        self.ffwd = FeedForward(n_embd, dropout)\n",
                "        self.ln1, self.ln2 = nn.LayerNorm(n_embd), nn.LayerNorm(n_embd)\n",
                "    def forward(self, x):\n",
                "        x = x + self.sa(self.ln1(x))\n",
                "        return x + self.ffwd(self.ln2(x))\n",
                "\n",
                "class GLTCH(nn.Module):\n",
                "    def __init__(self, vocab_size, n_embd=384, n_head=6, n_layer=6, block_size=256, dropout=0.2):\n",
                "        super().__init__()\n",
                "        self.block_size = block_size\n",
                "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
                "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
                "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)])\n",
                "        self.ln_f = nn.LayerNorm(n_embd)\n",
                "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
                "\n",
                "    def forward(self, idx): \n",
                "        B, T = idx.shape\n",
                "        x = self.tok_emb(idx) + self.pos_emb(torch.arange(T, device=idx.device))\n",
                "        return self.lm_head(self.ln_f(self.blocks(x)))\n",
                "\n",
                "    def generate(self, idx, max_tokens):\n",
                "        for _ in range(max_tokens):\n",
                "            logits = self(idx[:, -self.block_size:])[:, -1, :]\n",
                "            idx = torch.cat((idx, torch.multinomial(F.softmax(logits, dim=-1), 1)), dim=1)\n",
                "        return idx\n",
                "\n",
                "print('âœ… Model classes loaded!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title ğŸ’¬ Chat with GLTCH { run: \"auto\" }\n",
                "prompt = \"KING: To be or not\" #@param {type:\"string\"}\n",
                "max_tokens = 200 #@param {type:\"slider\", min:50, max:500, step:50}\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "checkpoint = torch.load('gltch_model.pt', map_location=device)\n",
                "\n",
                "stoi = checkpoint.get('stoi', {chr(i): i for i in range(128)})\n",
                "itos = {v: k for k, v in stoi.items()}\n",
                "\n",
                "model = GLTCH(len(stoi)).to(device)\n",
                "model.load_state_dict(checkpoint['model_state_dict'])\n",
                "model.eval()\n",
                "\n",
                "encode = lambda s: [stoi.get(c, 0) for c in s]\n",
                "decode = lambda l: ''.join([itos.get(i, '?') for i in l])\n",
                "\n",
                "with torch.no_grad():\n",
                "    context = torch.tensor([encode(prompt)], device=device)\n",
                "    output = decode(model.generate(context, max_tokens)[0].tolist())\n",
                "\n",
                "print('â•' * 50)\n",
                "print('ğŸ¤– GLTCH says:')\n",
                "print('â•' * 50)\n",
                "print(output)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ğŸ“Š Training Stats\n",
                "\n",
                "| Model Size | Parameters | VRAM | Speed |\n",
                "|------------|------------|------|-------|\n",
                "| 1m | ~1M | <1GB | Very Fast |\n",
                "| 2.7m | 2.7M | ~1GB | Fast |\n",
                "| 10m | ~10M | ~2GB | Normal |\n",
                "| 25m | ~25M | ~4GB | Slow |\n",
                "| 50m | ~50M | ~8GB | Slowest |\n",
                "\n",
                "---\n",
                "\n",
                "## ğŸ”— Links\n",
                "\n",
                "- **Dashboard:** https://hive.gltch.app\n",
                "- **GitHub:** https://github.com/cyberdreadx/gltch-llm\n",
                "- **Created by:** cyberdreadx"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "name": "GLTCH Hive Peer",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}